{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, shutil\n",
    "from ovejero import model_trainer, hierarchical_inference\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "\n",
    "def NOTIMPLEMENTED():\n",
    "    raise NotImplementedError('Must specify config/save path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Inference on a Test Set\n",
    "\n",
    "__Author:__ Sebastian Wagner-Carena\n",
    "\n",
    "__Last Run:__ 08/15/2020\n",
    "\n",
    "__Goals:__ Learn how to run hierarchical inference on a test set using a trained BNN\n",
    "\n",
    "__Before running this notebook:__ Train a model and generate a test set on which you want to run hiearchical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run hierarchical inference first we need to specify the path to the config files we'll use. There are three required configs for hierarchical inference:\n",
    "\n",
    "1. The ovejero bnn config used for training/validation/testing\n",
    "\n",
    "2. The ovejero distribution config that specifies the hyperparameters we'll run hierarchical inference over. For an example see the config in configs/baobab_configs/cent_narrow_cfg_prior.py.\n",
    "\n",
    "3. The baobab config used to geneate the training set.\n",
    "\n",
    "You can also optionally specify the baobab config used to generate the test set. This will be used to plot the true values of the hyperparameters you're trying to infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are also optional, but these are the names of the hyperparameters and parameters of the lens sample.\n",
    "# They will only be used in plotting.\n",
    "hyperparam_plot_names = [r'$\\mu_{\\log(\\gamma_\\mathrm{ext})}$',r'$\\sigma_{\\log(\\gamma_\\mathrm{ext})}$',\n",
    "                         r'$\\mu_x$',r'$\\sigma_x$',r'$\\mu_y$',r'$\\sigma_y$',\n",
    "                         r'$\\mu_{e1}$',r'$\\sigma_{e1}$',\n",
    "                         r'$\\mu_{e2}$',r'$\\sigma_{e2}$',\n",
    "                         r'$\\mu_{\\log (\\gamma_\\mathrm{lens})}$',r'$\\sigma_{\\log (\\gamma_\\mathrm{lens})}$',\n",
    "                         r'$\\mu_{\\log (\\theta_E)}$',r'$\\sigma_{\\log (\\theta_E)}$']\n",
    "param_plot_names = [r'$\\gamma_\\mathrm{ext}$', r'$\\psi_\\mathrm{ext}$',r'$x_\\mathrm{lens}$',\n",
    "            r'$y_\\mathrm{lens}$',r'$e_1$',r'$e_2$',r'$\\gamma_\\mathrm{lens}$',r'$\\theta_E$']\n",
    "\n",
    "# The config path used to train the BNN\n",
    "bnn_config_path = NOTIMPLEMENTED()\n",
    "bnn_cfg = model_trainer.load_config(bnn_config_path)\n",
    "\n",
    "def recursive_str_checker(cfg_dict):\n",
    "    for key in cfg_dict:\n",
    "        if isinstance(cfg_dict[key],str):\n",
    "            cfg_dict[key] = cfg_dict[key].replace('/home/swagnercarena/ovejero/',root_path)\n",
    "        if isinstance(cfg_dict[key],dict):\n",
    "            recursive_str_checker(cfg_dict[key])\n",
    "recursive_str_checker(bnn_cfg)\n",
    "\n",
    "# The baobab config used to generate the training set.\n",
    "interim_baobab_omega_path = NOTIMPLEMENTED()\n",
    "\n",
    "# The ovejero distribution config specifying the hyperparameters you want to fit.\n",
    "target_ovejero_omega_path = NOTIMPLEMENTED()\n",
    "\n",
    "# Optional, but you can also specify the baobab config used to generate the test set.\n",
    "target_baobab_omega_path = NOTIMPLEMENTED()\n",
    "\n",
    "# The path to the test dataset\n",
    "test_dataset_path = '/Users/sebwagner/Documents/Grad_School/Research/Phil/ovejero/datasets/cent_narrow/' # NOTIMPLEMENTED()\n",
    "\n",
    "# The path to which the tf record will be saved\n",
    "test_dataset_tf_record_path = NOTIMPLEMENTED()\n",
    "\n",
    "# The number of walkers to use in the hierarchical inference. This should be AT LEAST double the number of\n",
    "# hyperparameters that are being inferred.\n",
    "n_walkers = 50\n",
    "\n",
    "# If you've already generated the samples you can set this to True. If you do, the weights won't be\n",
    "# loaded, avoiding memory errors.\n",
    "lite_class = False\n",
    "\n",
    "# The HierarchicalClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the test dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "# To initialize it we need to pass in our config files\n",
    "hier_infer = hierarchical_inference.HierarchicalClass(bnn_cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                      test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                      target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                      lite_class=lite_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've initialized our class, we need to generate bnn samples for the lenses in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A path where the BNN samples will be saved\n",
    "save_path_samples = NOTIMPLEMENTED()\n",
    "\n",
    "# The number of BNN samples to draw per lens\n",
    "num_samples = 1000\n",
    "\n",
    "# This command will generate the samples on the test set\n",
    "hier_infer.gen_samples(num_samples,save_path_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the hierarchical inference. We have to specify the number of walkers and the path to save the emcee samples to. We'll pick a specific path for the demo that we'll clear out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of walkers\n",
    "n_walkers = 50\n",
    "\n",
    "# The path to save the emcee samples to\n",
    "save_path_chains_hr = NOTIMPLEMENTED()\n",
    "\n",
    "# Initialize the sampler\n",
    "hier_infer.initialize_sampler(n_walkers,save_path_chains_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run our hierarchical inference. For 100 steps this should take a few minutes (less when you have more cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can run the sampler for 100 steps.\n",
    "num_emcee_samples = 100\n",
    "hier_infer.run_sampler(num_emcee_samples,progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 steps isn't enough for convergence, but we can still inspect the chains. The HierarchicalInference class allows us to plot the chains and to do some basic autocorrelation analysis. Neither plot should make you feel like things have converged in 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 0\n",
    "hier_infer.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_infer.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class also allows us to inspect the results of the hierarchical inference and generate some nice plots. For an example check the hierarchical inference notebook in the papers folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the weights we generated.\n",
    "os.remove('demo_hier_samples.h5')\n",
    "shutil.rmtree('fow_model_bnn_samps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
