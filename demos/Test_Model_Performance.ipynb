{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "%matplotlib inline\n",
    "from ovejero import model_trainer, data_tools, bnn_inference\n",
    "import corner\n",
    "\n",
    "def NOTIMPLEMENTED():\n",
    "    raise NotImplementedError('Must specify config path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Performance of a Model That Has Been Fit\n",
    "\n",
    "__Author:__ Sebastian Wagner-Carena\n",
    "\n",
    "__Created:__ 11/14/2019\n",
    "\n",
    "__Last Run:__ 11/14/2019\n",
    "\n",
    "__Goals:__ Learn how to test the performance of a trained model on the validation set.\n",
    "\n",
    "__Before running this notebook:__ Run the Train_Toy_Model notebook to understand how to train a model. Then train a model with whatever configuration you want. You will have to add the path to the config file in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First specify the config path\n",
    "config_path = '../configs/t1_local.json'#NOTIMPLEMENTED()\n",
    "\n",
    "# Check that the config has what you need\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "\n",
    "# The InferenceClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the validation dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "bnn_infer = bnn_inference.InferenceClass(cfg)\n",
    "\n",
    "# Now we just have to ask the InferenceClass to spin up some samples from our BNN. The more samples, the more\n",
    "# accurate our plots and metrics will be. The right value to use unfortunately requires a bit of trial and error.\n",
    "# 1000 is a good starting point though.\n",
    "num_samples = 1000\n",
    "bnn_infer.gen_samples(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we set up our infastructure, the first thing we want to do is inspect the statistics of our network's performance over the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_infer.report_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect a coverage plot of our parameters. If our model is performing well, we expect our data to roughly follow the 68-95-99.7 rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_infer.gen_coverage_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good check is to see the posterior of some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 5\n",
    "bnn_infer.plot_posterior_contours(image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand where our uncertainty is coming from. We can inspect wether our uncertainty is dominated by aleatoric or epistemic sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_infer.comp_al_ep_unc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end what we want our network's posterior to be well calibrated. That means that the truth should be a representative draw from the distribution we're predicting. The exact sampling that goes into the calibration plot is complicated, but the x axis repesents how much of the data the model expects to fall within a certain region of our posterior, and the y axis represents how much data actually falls within that region. Ideally this would be a straight line (y=x), but in practice our model is likely to be overconfident, underconfident, or some combination of both. The lower right hand corner of our plot represents overconfidence, and the upper right hand corner represents underconfidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 10000\n",
    "color_map = [\"#377eb8\", \"#4daf4a\"]\n",
    "n_perc_points = 10\n",
    "fig = bnn_infer.plot_calibration(color_map=color_map,n_perc_points=n_perc_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could repeat the same plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better feel for the calibration plot, we can repeat the same analysis on some toy 2D models. We can start with a biased 2D posterior prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll make a class to generate our comparison\n",
    "def plot_toy_model_calibration(data_mean,data_cov,post_mean,post_cov,toy_batch_size,n_draws,\n",
    "                              fit_guass_data=False):\n",
    "    bnn_toy = bnn_inference.InferenceClass(cfg)\n",
    "    # We generate our toy data \n",
    "    data = np.random.multivariate_normal(data_mean,data_cov,(toy_batch_size))\n",
    "\n",
    "    # Now we generate our posterior means and covariances\n",
    "    post_samples = np.random.multivariate_normal(post_mean,post_cov,(n_draws,toy_batch_size))\n",
    "\n",
    "    # We change our bnn inference instance to have these values\n",
    "    bnn_toy.samples_init = True\n",
    "    bnn_toy.y_pred = np.mean(post_samples,axis=0)\n",
    "    bnn_toy.predict_samps = post_samples\n",
    "    bnn_toy.y_test = data\n",
    "    \n",
    "    # We can visualize the true data and the posterior, and compare that to the calibration plot.\n",
    "    color_map=[\"#377eb8\", \"#4daf4a\"]\n",
    "    fig = corner.corner(post_samples.reshape(-1,2),bins=20,labels=['x','y'],show_titles=False, plot_datapoints=False,\n",
    "                  label_kwargs=dict(fontsize=13),levels=[0.68,0.95],dpi=1600, \n",
    "                  color=color_map[0],fill_contours=True,range=[[-6,6],[-6,6]])\n",
    "    fig.axes[2].plot(data[:,0],data[:,1],'.',c=color_map[1],alpha=0.1)\n",
    "    data_line = mlines.Line2D([], [], color=color_map[0], label='Posterior')\n",
    "    post_line = mlines.Line2D([], [], color=color_map[1], label='Data')\n",
    "    plt.legend(handles=[data_line,post_line], bbox_to_anchor=(0., 1.0, 1., .0), loc=4,fontsize=15.0)\n",
    "    plt.show()\n",
    "    bnn_toy.plot_calibration(n_perc_points=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with our offset posterior\n",
    "data_mean = np.zeros(2)\n",
    "data_cov = np.eye(2)\n",
    "toy_batch_size = 10000\n",
    "n_draws = 1000\n",
    "post_mean = np.ones(2)*2\n",
    "post_cov=np.eye(2)\n",
    "plot_toy_model_calibration(data_mean,data_cov,post_mean,post_cov,toy_batch_size,n_draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior we're predicting is offset from the truth, so our model is consistently overconfident. We can repeat the exercise with a posterior that is correctly centered but has a much tighter contour. We still expect our model to be overconfident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = np.zeros(2)\n",
    "data_cov = np.eye(2)\n",
    "toy_batch_size = 10000\n",
    "n_draws = 1000\n",
    "post_mean = np.zeros(2)\n",
    "post_cov=np.eye(2)*0.3\n",
    "plot_toy_model_calibration(data_mean,data_cov,post_mean,post_cov,toy_batch_size,n_draws = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, our model is overconfident. We can similary see what happens when our model is underconfident by expanding our contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = np.zeros(2)\n",
    "data_cov = np.eye(2)\n",
    "toy_batch_size = 10000\n",
    "n_draws = 1000\n",
    "post_mean = np.zeros(2)\n",
    "post_cov=np.eye(2)*3\n",
    "plot_toy_model_calibration(data_mean,data_cov,post_mean,post_cov,toy_batch_size,n_draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model posterior here is underconfident - almost 90% of the data falls within the 1 sigma countour. We can look at a more realistic example - a Gaussian posterior with no covariance trying to fit data with covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with our offset posterior\n",
    "data_mean = np.zeros(2)\n",
    "data_cov = np.array([[1,0.99],[0.99,1]])\n",
    "toy_batch_size = 100000\n",
    "n_draws = 1000\n",
    "post_mean = np.zeros(2)\n",
    "post_cov=np.diag(np.std(np.random.multivariate_normal(data_mean,data_cov,(toy_batch_size)),axis=0))\n",
    "plot_toy_model_calibration(data_mean,data_cov,post_mean,post_cov,toy_batch_size,n_draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comes off mostly as underconfidence by our network - it's not capturing the extreme covariance in the data, causing the networks contours to be too large in the inner region. Note that for the largest percentiles our network is slightly overconfident - the extreme diagonality of our true distribution leads to many low probability points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue our network may have is that the posterior we pick is not sufficiently multimodal to capture the true distribution of the data (or the multimodality is poorly tuned). We can see what this looks like by fitting a full covariance matrix posterior to multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll make a class to generate our comparison\n",
    "def plot_toy_model_calibration_gm(data_means,data_covs,post_mean,post_cov,toy_batch_size,ps,n_draws,\n",
    "                              fit_guass_data=False):\n",
    "    bnn_toy = bnn_inference.InferenceClass(cfg)\n",
    "    # We generate our toy data \n",
    "    data = []\n",
    "    for dmi in range(len(data_means)):\n",
    "        data.append(np.random.multivariate_normal(data_means[dmi],data_covs[dmi],(int(toy_batch_size*ps[dmi]))))\n",
    "    data = np.concatenate(data,axis=0)\n",
    "    \n",
    "    if fit_guass_data == True:\n",
    "        post_mean = np.mean(data,axis=0)\n",
    "        post_cov=np.diag(np.std(data,axis=0))\n",
    "\n",
    "    # Now we generate our posterior means and covariances\n",
    "    post_samples = np.random.multivariate_normal(post_mean,post_cov,(n_draws,toy_batch_size))\n",
    "\n",
    "    # We change our bnn inference instance to have these values\n",
    "    bnn_toy.samples_init = True\n",
    "    bnn_toy.y_pred = np.mean(post_samples,axis=0)\n",
    "    bnn_toy.predict_samps = post_samples\n",
    "    bnn_toy.y_test = data\n",
    "    \n",
    "    # We can visualize the true data and the posterior, and compare that to the calibration plot.\n",
    "    color_map=[\"#377eb8\", \"#4daf4a\"]\n",
    "    fig = corner.corner(post_samples.reshape((-1,2)),bins=20,labels=['x','y'],show_titles=False, \n",
    "                        plot_datapoints=False,label_kwargs=dict(fontsize=13),levels=[0.68,0.95],dpi=1600, \n",
    "                        color=color_map[0],fill_contours=True,range=[[-6,6],[-6,6]])\n",
    "    fig.axes[2].plot(data[:,0],data[:,1],'.',c=color_map[1],alpha=0.1)\n",
    "    data_line = mlines.Line2D([], [], color=color_map[0], label='Posterior')\n",
    "    post_line = mlines.Line2D([], [], color=color_map[1], label='Data')\n",
    "    plt.legend(handles=[data_line,post_line], bbox_to_anchor=(0., 1.0, 1., .0), loc=4,fontsize=15.0)\n",
    "    plt.show()\n",
    "    bnn_toy.plot_calibration(n_perc_points=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with our offset posterior\n",
    "data_means = [np.ones(2)*3,np.zeros(2)]\n",
    "data_covs = [np.array([[0.4,0],[0,0.4]]),np.array([[0.4,0],[0,0.4]])]\n",
    "ps = [0.9,0.1]\n",
    "toy_batch_size = 10000\n",
    "n_draws = 1000\n",
    "\n",
    "data = []\n",
    "for dmi in range(len(data_means)):\n",
    "    data.append(np.random.multivariate_normal(data_means[dmi],data_covs[dmi],(toy_batch_size//len(\n",
    "    data_mean))))\n",
    "data = np.concatenate(data,axis=0)\n",
    "\n",
    "post_mean = np.mean(data,axis=0)\n",
    "post_cov=np.diag(np.std(data,axis=0))\n",
    "plot_toy_model_calibration_gm(data_means,data_covs,post_mean,post_cov,toy_batch_size,ps,n_draws,fit_guass_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with our offset posterior\n",
    "data_means = [np.ones(2)*3,np.zeros(2)]\n",
    "data_covs = [np.array([[0.4,0],[0,0.4]]),np.array([[0.4,0],[0,0.4]])]\n",
    "ps = [0.7,0.3]\n",
    "toy_batch_size = 10000\n",
    "n_draws = 1000\n",
    "\n",
    "data = []\n",
    "for dmi in range(len(data_means)):\n",
    "    data.append(np.random.multivariate_normal(data_means[dmi],data_covs[dmi],(toy_batch_size//len(\n",
    "    data_mean))))\n",
    "data = np.concatenate(data,axis=0)\n",
    "\n",
    "post_mean = np.mean(data,axis=0)\n",
    "post_cov=np.diag(np.std(data,axis=0))\n",
    "plot_toy_model_calibration_gm(data_means,data_covs,post_mean,post_cov,toy_batch_size,ps,n_draws,fit_guass_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
