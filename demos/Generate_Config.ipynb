{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from ovejero import model_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a Configuration File for Training a Model\n",
    "\n",
    "__Author:__ Sebastian Wagner-Carena\n",
    "\n",
    "__Last Run:__ 08/04/2020\n",
    "\n",
    "__Goals:__ Learn how to use json function in python to write out a configuration file for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four dictionaries associated with training the bnn models in ovejero. \n",
    "\n",
    "1) **training_params**: The parameters used for the optimization and training of the model. \n",
    "\n",
    "2) **validation_params**: The parameters used to process the validation set.\n",
    "\n",
    "3) **dataset_params**: The parameters used to process the dataset and generate the TFRecord file.\n",
    "\n",
    "4) **inference_params**: The parameters used at inference time (mainly plotting names)\n",
    "\n",
    "5) **forward_mod_params**: The parameters used to compare to the forward modeling outputs\n",
    "\n",
    "We will start by setting the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the base dictionary\n",
    "training_params = {}\n",
    "\n",
    "# Probably the most important parameters - the type of BNN we want to train\n",
    "training_params['bnn_type'] = 'gmm'\n",
    "training_params['dropout_type'] = 'standard'\n",
    "\n",
    "# First the optimization specific parameters\n",
    "training_params['batch_size'] = 10\n",
    "training_params['n_epochs'] = 10\n",
    "training_params['learning_rate'] = 1e-4\n",
    "training_params['decay'] = 0.000003\n",
    "training_params['kernel_regularizer'] = 1e-5\n",
    "training_params['dropout_rate'] = 0.1\n",
    "\n",
    "# We still have to populate the dropout regularizer in the dictionary, although \n",
    "# we will not use it for standard dropout.\n",
    "training_params['dropout_regularizer'] = 1e-6\n",
    "\n",
    "# Now set the parameters that will point us to the tf_record\n",
    "# For this example we want to set the path to the test files\n",
    "training_params['root_path'] = './test_data/'\n",
    "training_params['tf_record_path'] = 'tf_record_test'\n",
    "\n",
    "# Set the final list of parameters we want to use for analysis\n",
    "training_params['final_params'] = ['external_shear_g1','external_shear_g2','lens_mass_center_x',\n",
    "                                   'lens_mass_center_y','lens_mass_e1','lens_mass_e2','lens_mass_gamma',\n",
    "                                   'lens_mass_theta_E_log']\n",
    "# Using the same ordering as final_params, we also want to set the flip_pairs. If using lenstronomy convention\n",
    "# this does not need to be used. Specify an empty list.\n",
    "training_params['flip_pairs'] = []\n",
    "# The dimensions of the images\n",
    "training_params['img_dim'] = 128\n",
    "\n",
    "# The path to load/save the model weights to for this configuration.\n",
    "training_params['model_weights'] = training_params['root_path']+'test_model.h5'\n",
    "\n",
    "# Where to save the tensorboard logs to\n",
    "training_params['tensorboard_log_dir'] = training_params['root_path']+'test.log'\n",
    "\n",
    "# Set the random seed at train time for reproducibility!\n",
    "training_params['random_seed'] = 1138\n",
    "\n",
    "# Set the augmentation parameters\n",
    "# A boolean that dictates whether or not the images should be normalzied to have standard deviation 1\n",
    "training_params['norm_images'] = True\n",
    "# The number of pixels to uniformly shift the images by. If set to 0 no shifting will occur\n",
    "training_params['shift_pixels'] = 2\n",
    "# A tuple of lists that contain the x and y parameters that need to be rescaled to account for this shift\n",
    "training_params['shift_params'] = (['lens_mass_center_x'],['lens_mass_center_y'])\n",
    "# What the pixel_scale of the images is.\n",
    "training_params['pixel_scale'] = 0.051\n",
    "\n",
    "# The path to the baobab configuration file used to generate the training data. Noise will be added on the fly\n",
    "# using the noise parameters in this file.\n",
    "training_params['baobab_config_path'] =  training_params['root_path']+'test_baobab_cfg.py'\n",
    "\n",
    "# Make sure we've set the right number of training parameters (not a full proof check)\n",
    "if len(training_params) < 22:\n",
    "    raise RuntimeError('Missing something in training_params!')\n",
    "elif len(training_params) > 22:\n",
    "    raise RuntimeError('Too much stuff in training_params!')\n",
    "else:\n",
    "    print('All set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to set up the **val_params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the base dictionary\n",
    "val_params = {}\n",
    "\n",
    "# All we really need to set here is the path to the validation data and the desired output path to the TFRecord.\n",
    "# The rest can just be pulled from the training_params.\n",
    "val_params['root_path'] = './test_data/'\n",
    "val_params['tf_record_path'] = 'tf_record_test_val'\n",
    "\n",
    "# Make sure we've set the right number of validation parameters (not a full proof check)\n",
    "if len(val_params) < 2:\n",
    "    raise RuntimeError('Missing something in training_params!')\n",
    "elif len(val_params) > 2:\n",
    "    raise RuntimeError('Too much stuff in training_params!')\n",
    "else:\n",
    "    print('All set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to repeat the same for the **dataset_params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the base dictionary\n",
    "dataset_params = {}\n",
    "\n",
    "# First set path for the old and new lensing parameters. If you don't mind rewriting the old file, these can be the\n",
    "# same.\n",
    "dataset_params['lens_params_path'] = 'metadata.csv'\n",
    "dataset_params['new_param_path'] = 'new_metadata.csv'\n",
    "# Also set where to save the normalization weights to\n",
    "dataset_params['normalization_constants_path'] = 'norms.csv'\n",
    "\n",
    "# Set the lens parameters to be pulled from metadata.csv.\n",
    "dataset_params['lens_params'] = ['external_shear_gamma_ext','external_shear_psi_ext','lens_mass_center_x',\n",
    "                                 'lens_mass_center_y','lens_mass_e1','lens_mass_e2','lens_mass_gamma',\n",
    "                                 'lens_mass_theta_E']\n",
    "\n",
    "# List of parameters that need to be converted to log space\n",
    "dataset_params['lens_params_log'] = ['lens_mass_theta_E']\n",
    "\n",
    "# List of parameters that need to be converted from radius and angle to cartesian coordinates\n",
    "dataset_params['gampsi'] = {}\n",
    "dataset_params['gampsi']['gampsi_parameter_prefixes'] = ['external_shear']\n",
    "dataset_params['gampsi']['gampsi_params_rat'] = ['external_shear_gamma_ext']\n",
    "dataset_params['gampsi']['gampsi_params_ang'] = ['external_shear_psi_ext']\n",
    "\n",
    "# Make sure we've set the right number of dataset parameters (not a full proof check)\n",
    "if len(dataset_params) < 6:\n",
    "    raise RuntimeError('Missing something in dataset_params!')\n",
    "elif len(dataset_params) > 6:\n",
    "    raise RuntimeError('Too much stuff in dataset_params!')\n",
    "elif len(dataset_params['gampsi']) != 3:\n",
    "    raise RuntimeError('Gamma and angle parameters are off!')\n",
    "else:\n",
    "    print('All set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some parameters we use to make our plots at inference time in **inference_params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the base dictionary\n",
    "inference_params = {}\n",
    "\n",
    "# The pyplot names that will be used for our final parameters.\n",
    "inference_params['final_params_print_names'] = ['$\\gamma_1$','$\\gamma_2$','$x_\\mathrm{lens}$',\n",
    "                                   '$y_\\mathrm{lens}$','$e_1$','$e_2$','$\\gamma_\\mathrm{lens}$',\n",
    "                                   '$\\log(\\\\theta_E)$']\n",
    "\n",
    "# Make sure we've set the right number of inference parameters (not a full proof check)\n",
    "if len(inference_params) < 1:\n",
    "    raise RuntimeError('Missing something in dataset_params!')\n",
    "elif len(inference_params) > 1:\n",
    "    raise RuntimeError('Too much stuff in dataset_params!')\n",
    "else:\n",
    "    print('All set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be interested in doing forward modeling on a few lenses in your validation set to compare the BNN outputs to those of a more traditional pipeline. Ovejero uses Lenstronomy and Baobab for this, but a few extra user-level parameters need to be specified in __forward_mod_params__. If you're not interested in this funcitonality, feel free to set these parameters to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the base dictionary\n",
    "forward_mod_params = {}\n",
    "\n",
    "# The list of lens model. Check forward_modeling.py to see what is supported.\n",
    "forward_mod_params['lens_model_list'] = ['PEMD','SHEAR_GAMMA_PSI']\n",
    "\n",
    "# The list of source models.\n",
    "forward_mod_params['source_model_list'] = ['SERSIC_ELLIPSE']\n",
    "\n",
    "# Make sure we've set the right number of forward modeling parameters (not a full proof check)\n",
    "if len(forward_mod_params) < 2:\n",
    "    raise RuntimeError('Missing something in dataset_params!')\n",
    "elif len(forward_mod_params) > 2:\n",
    "    raise RuntimeError('Too much stuff in dataset_params!')\n",
    "else:\n",
    "    print('All set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can combine this into our configuration dictionairy and write it to a json file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '../test/test_data/' + 'test.json'\n",
    "config_dict = {'training_params':training_params, 'validation_params':val_params,'dataset_params':dataset_params,\n",
    "              'inference_params':inference_params, 'forward_mod_params':forward_mod_params}\n",
    "# Final check that the config file meets requirements\n",
    "model_trainer.config_checker(config_dict)\n",
    "with open(json_path,'w') as json_f:\n",
    "    json.dump(config_dict,json_f,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
