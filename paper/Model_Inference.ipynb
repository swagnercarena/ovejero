{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from ovejero import model_trainer, data_tools, bnn_inference\n",
    "\n",
    "# Modifies the paths in the config to agree with the paths being used on the current computer.\n",
    "def recursive_str_checker(cfg_dict):\n",
    "    for key in cfg_dict:\n",
    "        if isinstance(cfg_dict[key],str):\n",
    "            cfg_dict[key] = cfg_dict[key].replace('/home/swagnercarena/ovejero/',root_path)\n",
    "        if isinstance(cfg_dict[key],dict):\n",
    "            recursive_str_checker(cfg_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Performance of Trained Models\n",
    "\n",
    "__Author:__ Sebastian Wagner-Carena\n",
    "\n",
    "__Last Run:__ 07/27/2020\n",
    "\n",
    "__Goals:__ Inspect how the different BNN model types and dropout rates impact performance metrics.\n",
    "\n",
    "__Before running this notebook:__ You will have to download and unzip the bnn validation samples that can be found here (TODO). Because we already have the BNN samples, the model weights are not neccesary. If you would rather regenerate the results from the weights, simply do not download the weights and not the samples. This pipeline will automatically rerun the sampling and save it to the specified path. I would not recommend this without a GPU (it could easily take over an hour on a CPU). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Posterior Calibration and MAD Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to compare the calibration and mean absolute deviation for our diagonal BNN models. Here, we will look at a 30%, 10%, and 5% dropout model. The first step is to load all of the inference classes required to make our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First specify the config path\n",
    "root_path = os.getcwd()[:-5]\n",
    "config_path_nn1_30 = root_path + 'configs/nn1_hr.json'\n",
    "config_path_nn1_10 = root_path + 'configs/nn1.json'\n",
    "config_path_nn1_5 = root_path + 'configs/nn1_lr.json'\n",
    "\n",
    "# Load the configs and then fix the paths.\n",
    "cfg_nn1_30 = model_trainer.load_config(config_path_nn1_30)\n",
    "cfg_nn1_10 = model_trainer.load_config(config_path_nn1_10)\n",
    "cfg_nn1_5 = model_trainer.load_config(config_path_nn1_5)\n",
    "\n",
    "recursive_str_checker(cfg_nn1_30)\n",
    "recursive_str_checker(cfg_nn1_10)\n",
    "recursive_str_checker(cfg_nn1_5)\n",
    "\n",
    "# We don't need the model, we already have the samples\n",
    "lite_class = True\n",
    "\n",
    "# The InferenceClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the validation dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "bnn_infer_nn1_30 = bnn_inference.InferenceClass(cfg_nn1_30,lite_class=lite_class)\n",
    "bnn_infer_nn1_10 = bnn_inference.InferenceClass(cfg_nn1_10,lite_class=lite_class)\n",
    "bnn_infer_nn1_5 = bnn_inference.InferenceClass(cfg_nn1_5,lite_class=lite_class)\n",
    "\n",
    "# Now we just have to ask the InferenceClass to spin up some samples from our BNN. All of these samples should\n",
    "# already be saved.\n",
    "num_samples = 1000\n",
    "save_path_nn1_30 = root_path + 'validation_results/nn1_hr_samps/'\n",
    "save_path_nn1_10 = root_path + 'validation_results/nn1_samps/'\n",
    "save_path_nn1_5 = root_path + 'validation_results/nn1_lr_samps/'\n",
    "\n",
    "bnn_infer_nn1_30.gen_samples(num_samples,save_path_nn1_30)\n",
    "bnn_infer_nn1_10.gen_samples(num_samples,save_path_nn1_10)\n",
    "bnn_infer_nn1_5.gen_samples(num_samples,save_path_nn1_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the MAD for each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Diagonal 30% Model')\n",
    "bnn_infer_nn1_30.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn1_30.y_pred)-np.exp(bnn_infer_nn1_30.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('Diagonal 10% Model')\n",
    "bnn_infer_nn1_10.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn1_10.y_pred)-np.exp(bnn_infer_nn1_10.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('Diagonal 5% Model')\n",
    "bnn_infer_nn1_5.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn1_5.y_pred)-np.exp(bnn_infer_nn1_5.y_test)),\n",
    "                                     axis=0)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, we're interested in comparing the aleatoric and total uncertainty for the Diagonal 30% model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For apj, title was set to False.\n",
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "figures = bnn_infer_nn1_30.comp_al_ep_unc(norm_diagonal=False,title=False)\n",
    "figures[0].savefig('figures/diag_hr_cov_comp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make the calibration plot with our three dropout rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_map = ['#000000','#1b9e77','#32856c','#3d6b5d']\n",
    "ls_list =['-','--',':']\n",
    "n_perc_points = 30\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 13})\n",
    "fig = bnn_infer_nn1_5.plot_calibration(color_map=color_map,n_perc_points=n_perc_points,show_plot=False,ls=ls_list[0])\n",
    "fig = bnn_infer_nn1_10.plot_calibration(color_map=color_map[1:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False,ls=ls_list[1])\n",
    "fig = bnn_infer_nn1_30.plot_calibration(color_map=color_map[2:],n_perc_points=n_perc_points,figure=fig,show_plot=False,\n",
    "                                        legend=['Perfect Calibration',r'Dropout = 5%',r'Dropout = 10%',\n",
    "                                                r'Dropout = 30%'],\n",
    "                                        title='',ls=ls_list[2])\n",
    "\n",
    "save_cal_path = 'figures/diag_cal.pdf'\n",
    "plt.savefig(save_cal_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each inference class loads the model weights into memory, we're going to want to delete the ones we're not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bnn_infer_nn1_10\n",
    "del bnn_infer_nn1_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Posterior Calibration and MAD Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing now on the full posterior, we can once again load our validation samples for our 1%, 0.5%, 0.1%, and 0% dropout models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First specify the config path\n",
    "config_path_nn2_1 = root_path + 'configs/nn2.json'\n",
    "config_path_nn2_05 = root_path + 'configs/nn2_lr.json'\n",
    "config_path_nn2_01 = root_path + 'configs/nn2_slr.json'\n",
    "config_path_nn2_0 = root_path + 'configs/nn2_zr.json'\n",
    "\n",
    "# Load the configs and then fix the paths.\n",
    "cfg_nn2_1 = model_trainer.load_config(config_path_nn2_1)\n",
    "cfg_nn2_05 = model_trainer.load_config(config_path_nn2_05)\n",
    "cfg_nn2_01 = model_trainer.load_config(config_path_nn2_01)\n",
    "cfg_nn2_0 = model_trainer.load_config(config_path_nn2_0)\n",
    "\n",
    "recursive_str_checker(cfg_nn2_1)\n",
    "recursive_str_checker(cfg_nn2_05)\n",
    "recursive_str_checker(cfg_nn2_01)\n",
    "recursive_str_checker(cfg_nn2_0)\n",
    "\n",
    "# The InferenceClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the validation dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "bnn_infer_nn2_1 = bnn_inference.InferenceClass(cfg_nn2_1,lite_class=lite_class)\n",
    "bnn_infer_nn2_05 = bnn_inference.InferenceClass(cfg_nn2_05,lite_class=lite_class)\n",
    "bnn_infer_nn2_01 = bnn_inference.InferenceClass(cfg_nn2_01,lite_class=lite_class)\n",
    "bnn_infer_nn2_0 = bnn_inference.InferenceClass(cfg_nn2_0,lite_class=lite_class)\n",
    "\n",
    "# Now we just have to ask the InferenceClass to spin up some samples from our BNN. All of these samples should\n",
    "# already be saved.\n",
    "num_samples = 1000\n",
    "save_path_nn2_1 = root_path + 'validation_results/nn2_samps/'\n",
    "save_path_nn2_05 = root_path + 'validation_results/nn2_lr_samps/'\n",
    "save_path_nn2_01 = root_path + 'validation_results/nn2_slr_samps/'\n",
    "save_path_nn2_0 = root_path + 'validation_results/nn2_zr_samps/'\n",
    "\n",
    "bnn_infer_nn2_1.gen_samples(num_samples,save_path_nn2_1)\n",
    "bnn_infer_nn2_05.gen_samples(num_samples,save_path_nn2_05)\n",
    "bnn_infer_nn2_01.gen_samples(num_samples,save_path_nn2_01)\n",
    "bnn_infer_nn2_0.gen_samples(num_samples,save_path_nn2_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the MAD performance for our parameters on the full posterior BNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full 1% Model')\n",
    "bnn_infer_nn2_1.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn2_1.y_pred)-np.exp(bnn_infer_nn2_1.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('Full 0.5% Model')\n",
    "bnn_infer_nn2_05.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn2_05.y_pred)-np.exp(bnn_infer_nn2_05.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('Full 0.1% Model')\n",
    "bnn_infer_nn2_01.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn2_01.y_pred)-np.exp(bnn_infer_nn2_01.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('Full 0% Model')\n",
    "bnn_infer_nn2_0.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn2_0.y_pred)-np.exp(bnn_infer_nn2_0.y_test)),\n",
    "                                     axis=0)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper we're interested in comparing the aleatoric and total uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "figures = bnn_infer_nn2_01.comp_al_ep_unc(norm_diagonal=False,title=False)\n",
    "figures[0].savefig('figures/full_slr_cov_comp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full models, we can make the same calibration plot comparison as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['#000000','#d95f02','#cc6b21','#c0753b','#b37c52']\n",
    "ls_list =['-.','-','--',':']\n",
    "n_perc_points = 30\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 13})\n",
    "fig = bnn_infer_nn2_0.plot_calibration(color_map=color_map,n_perc_points=n_perc_points,show_plot=False,\n",
    "                                       ls=ls_list[0])\n",
    "fig = bnn_infer_nn2_01.plot_calibration(color_map=color_map[1:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False,ls=ls_list[1])\n",
    "fig = bnn_infer_nn2_05.plot_calibration(color_map=color_map[2:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False,ls=ls_list[2])\n",
    "fig = bnn_infer_nn2_1.plot_calibration(color_map=color_map[3:],n_perc_points=n_perc_points,figure=fig,show_plot=False,\n",
    "                                       legend=['Perfect Calibration',r'No Dropout',r'Dropout = 0.1%',\n",
    "                                               r'Dropout = 0.5%',r'Dropout = 1%'],\n",
    "                                       title='',ls=ls_list[3])\n",
    "\n",
    "save_cal_path = 'figures/full_cal.pdf'\n",
    "plt.savefig(save_cal_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will delete the models we will not be using anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bnn_infer_nn2_0\n",
    "del bnn_infer_nn2_05\n",
    "del bnn_infer_nn2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Posterior Calibration and MAD Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the GMM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First specify the config path\n",
    "config_path_nn3_1 = root_path + 'configs/nn3.json'\n",
    "config_path_nn3_05 = root_path + 'configs/nn3_lr.json'\n",
    "config_path_nn3_01 = root_path + 'configs/nn3_slr.json'\n",
    "config_path_nn3_0 = root_path + 'configs/nn3_zr.json'\n",
    "\n",
    "# Load the configs and then fix the paths.\n",
    "cfg_nn3_1 = model_trainer.load_config(config_path_nn3_1)\n",
    "cfg_nn3_05 = model_trainer.load_config(config_path_nn3_05)\n",
    "cfg_nn3_01 = model_trainer.load_config(config_path_nn3_01)\n",
    "cfg_nn3_0 = model_trainer.load_config(config_path_nn3_0)\n",
    "\n",
    "recursive_str_checker(cfg_nn3_1)\n",
    "recursive_str_checker(cfg_nn3_05)\n",
    "recursive_str_checker(cfg_nn3_01)\n",
    "recursive_str_checker(cfg_nn3_0)\n",
    "\n",
    "# The InferenceClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the validation dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "bnn_infer_nn3_1 = bnn_inference.InferenceClass(cfg_nn3_1,lite_class=lite_class)\n",
    "bnn_infer_nn3_05 = bnn_inference.InferenceClass(cfg_nn3_05,lite_class=lite_class)\n",
    "bnn_infer_nn3_01 = bnn_inference.InferenceClass(cfg_nn3_01,lite_class=lite_class)\n",
    "bnn_infer_nn3_0 = bnn_inference.InferenceClass(cfg_nn3_0,lite_class=lite_class)\n",
    "\n",
    "# Now we just have to ask the InferenceClass to spin up some samples from our BNN. All of these samples should\n",
    "# already be saved.\n",
    "num_samples = 1000\n",
    "save_path_nn3_1 = root_path + 'validation_results/nn3_samps/'\n",
    "save_path_nn3_05 = root_path + 'validation_results/nn3_lr_samps/'\n",
    "save_path_nn3_01 = root_path + 'validation_results/nn3_slr_samps/'\n",
    "save_path_nn3_0 = root_path + 'validation_results/nn3_zr_samps/'\n",
    "\n",
    "bnn_infer_nn3_1.gen_samples(num_samples,save_path_nn3_1)\n",
    "bnn_infer_nn3_05.gen_samples(num_samples,save_path_nn3_05)\n",
    "bnn_infer_nn3_01.gen_samples(num_samples,save_path_nn3_01)\n",
    "bnn_infer_nn3_0.gen_samples(num_samples,save_path_nn3_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the MAD performance of the GMM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GMM 1% Model')\n",
    "bnn_infer_nn3_1.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn3_1.y_pred)-np.exp(bnn_infer_nn3_1.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('GMM 0.5% Model')\n",
    "bnn_infer_nn3_05.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn3_05.y_pred)-np.exp(bnn_infer_nn3_05.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('GMM 0.1% Model')\n",
    "bnn_infer_nn3_01.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn3_01.y_pred)-np.exp(bnn_infer_nn3_01.y_test)),\n",
    "                                     axis=0)[-1])\n",
    "print('')\n",
    "\n",
    "print('GMM 0% Model')\n",
    "bnn_infer_nn3_0.report_stats()\n",
    "print('lens_mass_theta_E:',np.median(np.abs(np.exp(bnn_infer_nn3_0.y_pred)-np.exp(bnn_infer_nn3_0.y_test)),\n",
    "                                     axis=0)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GMM models, we can make the same calibration plot comparison as for the full and diagonal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['#000000','#7570b3','#554fa6','#393299','#21198d']\n",
    "ls_list =['-.','-','--',':']\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "fig = bnn_infer_nn3_0.plot_calibration(color_map=color_map,n_perc_points=n_perc_points,show_plot=False,\n",
    "                                       ls=ls_list[0])\n",
    "fig = bnn_infer_nn3_01.plot_calibration(color_map=color_map[1:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False,ls=ls_list[1])\n",
    "fig = bnn_infer_nn3_05.plot_calibration(color_map=color_map[2:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False,ls=ls_list[2])\n",
    "fig = bnn_infer_nn3_1.plot_calibration(color_map=color_map[3:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                       show_plot=False,legend=['Perfect Calibration',r'No Dropout',r'Dropout = 0.1%',\n",
    "                                                               r'Dropout = 0.5%',r'Dropout = 1%'],\n",
    "                                       title='',ls=ls_list[3])\n",
    "\n",
    "save_cal_path = 'figures/gmm_cal.pdf'\n",
    "plt.savefig(save_cal_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will delete any models we're not using for memory conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bnn_infer_nn3_0\n",
    "del bnn_infer_nn3_05\n",
    "del bnn_infer_nn3_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Calibration for All Three Model Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have all three models of interest loaded, so it's just a matter of doing the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['#000000','#1b9e77','#d95f02','#7570b3','#e7298a']\n",
    "n_perc_points = 30\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 13})\n",
    "fig = bnn_infer_nn1_30.plot_calibration(color_map=color_map,n_perc_points=n_perc_points,show_plot=False)\n",
    "fig = bnn_infer_nn2_01.plot_calibration(color_map=color_map[1:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False)\n",
    "fig = bnn_infer_nn3_01.plot_calibration(color_map=color_map[2:],n_perc_points=n_perc_points,figure=fig,\n",
    "                                        show_plot=False,legend=['Perfect Calibration',r'Diagonal Calibration (30%)',\n",
    "                                                                r'Full Calibration (0.1%)',r'GMM Calibration (0.1%)'],\n",
    "                                       title = '')\n",
    "\n",
    "save_cal_path = 'figures/all_cal.pdf'\n",
    "plt.savefig(save_cal_path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
