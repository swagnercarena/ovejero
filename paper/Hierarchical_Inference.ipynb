{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from ovejero import model_trainer, hierarchical_inference\n",
    "from matplotlib.lines import Line2D\n",
    "import corner, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lenstronomy.Util.param_util import ellipticity2phi_q\n",
    "\n",
    "# This allows you to modify the paths in the config to agree with the path you're using\n",
    "def recursive_str_checker(cfg_dict):\n",
    "    for key in cfg_dict:\n",
    "        if isinstance(cfg_dict[key],str):\n",
    "            cfg_dict[key] = cfg_dict[key].replace('/home/swagnercarena/ovejero/',root_path)\n",
    "        if isinstance(cfg_dict[key],dict):\n",
    "            recursive_str_checker(cfg_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Hierarchical Inference Plots\n",
    "\n",
    "__Author:__ Sebastian Wagner-Carena\n",
    "\n",
    "__Last Run:__ 07/27/2020\n",
    "\n",
    "__Goals:__ Generate all of the hierarchical inference plots used in the paper analysis.\n",
    "\n",
    "__Before running this notebook:__ You will have to download and unzip the bnn samples, chains, and datasets that can be found here (TODO). Because we already have the BNN samples, the model weights are not neccesary. If you would like to reproduce these results starting from the model weights, see the demo notebook on hierarchical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a corner plot to compare training and test distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load each one of the metadatas to compare the corner plots\n",
    "root_path = os.getcwd()[:-5]\n",
    "train_metadata = pd.read_csv(root_path+'datasets/train/metadata.csv')\n",
    "cn_metadata = pd.read_csv(root_path+'datasets/cent_narrow/metadata.csv')\n",
    "sn_metadata = pd.read_csv(root_path+'datasets/shifted_narrow/metadata.csv')\n",
    "emp_metadata = pd.read_csv(root_path+'datasets/empirical/metadata.csv')\n",
    "\n",
    "# Specify the lens_params we're interested in\n",
    "config_path = root_path + 'configs/nn1_hr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "lens_params = cfg['dataset_params']['lens_params']\n",
    "final_params_print_names = cfg['inference_params']['final_params_print_names']\n",
    "final_params_print_names.pop(lens_params.index('external_shear_psi_ext'))\n",
    "final_params_print_names[0] = r'$\\gamma_\\mathrm{ext}$'\n",
    "lens_params.pop(lens_params.index('external_shear_psi_ext'))\n",
    "final_params_print_names[-1] = r'$\\theta_E$'\n",
    "\n",
    "# Get the samples in numpy format\n",
    "train_samps = train_metadata[lens_params].to_numpy()\n",
    "cn_samps = cn_metadata[lens_params].to_numpy()\n",
    "sn_samps = sn_metadata[lens_params].to_numpy()\n",
    "emp_samps = emp_metadata[lens_params].to_numpy()\n",
    "\n",
    "# Generate the plot\n",
    "figure= None\n",
    "color_map_distributions = ['000000','#e7298a','#66a61e','#e6ab02']\n",
    "plot_limits = [[0,0.8],[-0.3,0.3],[-0.3,0.3],[-0.6,0.6],[-0.6,0.6],[1.5,2.8],[0.6,1.4]]\n",
    "hist_limit = [20,10,10,10,10,15,20]\n",
    "smooth = 0.5\n",
    "fontsize = 16\n",
    "hist_kwargs = {'density':True,'color':color_map_distributions[0],'lw':2}\n",
    "figure = corner.corner(train_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[0],levels=[0.68,0.95],\n",
    "                       fill_contours=True,fig=figure,range=plot_limits,smooth=smooth,hist_kwargs=hist_kwargs)\n",
    "hist_kwargs['color']=color_map_distributions[1]\n",
    "figure = corner.corner(cn_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[1],levels=[0.68,0.95],\n",
    "                       fill_contours=True,fig=figure,range=plot_limits,smooth=smooth,hist_kwargs=hist_kwargs)\n",
    "hist_kwargs['color']=color_map_distributions[2]\n",
    "figure = corner.corner(sn_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[2],levels=[0.68,0.95],\n",
    "                       fill_contours=True,fig=figure,range=plot_limits,smooth=smooth,hist_kwargs=hist_kwargs)\n",
    "hist_kwargs['color']=color_map_distributions[3]\n",
    "figure = corner.corner(emp_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[3],levels=[0.68,0.95],\n",
    "                       fill_contours=True,fig=figure,range=plot_limits,smooth=smooth*2,hist_kwargs=hist_kwargs)\n",
    "handles = [Line2D([0], [0], color=color_map_distributions[0], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[1], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[2], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[3], lw=5)]\n",
    "figure.legend(handles,[r'Training Distribution', r'Centered Test Set',r'Shifted Test Set',r'Empirical Test Set'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "\n",
    "axes = np.array(figure.axes).reshape((len(lens_params),len(lens_params)))\n",
    "for i in range(len(lens_params)):\n",
    "    axes[i,i].set_ylim(0,hist_limit[i])\n",
    "    \n",
    "plt.savefig('figures/distribution_comp.pdf')\n",
    "plt.show()\n",
    "\n",
    "print('Percent of Training Points in Centered Test Set: ',\n",
    "      np.mean(np.prod((train_samps > np.min(cn_samps,axis=0)) * (train_samps < np.max(cn_samps,axis=0)),axis=1))*100)\n",
    "print('Number of Training Points in Centered Test Set: ',\n",
    "      np.sum(np.prod((train_samps > np.min(cn_samps,axis=0)) * (train_samps < np.max(cn_samps,axis=0)),axis=1)))\n",
    "print('Percent of Training Points in Shifted Test Set: ',\n",
    "      np.mean(np.prod((train_samps > np.min(sn_samps,axis=0)) * (train_samps < np.max(sn_samps,axis=0)),axis=1))*100)\n",
    "print('Number of Training Points in Shifted Test Set: ',\n",
    "      np.sum(np.prod((train_samps > np.min(sn_samps,axis=0)) * (train_samps < np.max(sn_samps,axis=0)),axis=1)))\n",
    "print('Percent of Training Points in Empirical Test Set: ',\n",
    "      np.mean(np.prod((train_samps > np.min(emp_samps,axis=0)) * (train_samps < np.max(emp_samps,axis=0)),axis=1))\n",
    "      *100)\n",
    "print('Number of Training Points in Empirical Test Set: ',\n",
    "      np.sum(np.prod((train_samps > np.min(emp_samps,axis=0)) * (train_samps < np.max(emp_samps,axis=0)),axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance of all three model types on the centered narrow test distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start by loading the files we'll need for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll define a few lists we'll use throughout plotting\n",
    "hyperparam_plot_names = [r'$\\mu_{\\log(\\gamma_\\mathrm{ext})}$',r'$\\sigma_{\\log(\\gamma_\\mathrm{ext})}$',\n",
    "                         r'$\\mu_x$',r'$\\sigma_x$',r'$\\mu_y$',r'$\\sigma_y$',\n",
    "                         r'$\\mu_{e1}$',r'$\\sigma_{e1}$',\n",
    "                         r'$\\mu_{e2}$',r'$\\sigma_{e2}$',\n",
    "                         r'$\\mu_{\\log (\\gamma_\\mathrm{lens})}$',r'$\\sigma_{\\log (\\gamma_\\mathrm{lens})}$',\n",
    "                         r'$\\mu_{\\log (\\theta_E)}$',r'$\\sigma_{\\log (\\theta_E)}$']\n",
    "param_plot_names = [r'$\\gamma_\\mathrm{ext}$', r'$\\psi_\\mathrm{ext}$',r'$x_\\mathrm{lens}$',\n",
    "            r'$y_\\mathrm{lens}$',r'$e_1$',r'$e_2$',r'$\\gamma_\\mathrm{lens}$',r'$\\theta_E$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal model 30% dropout chains on centered test distribution\n",
    "# Specify the paths for our distribution configs, the test set, and the location on the saved BNN samples and chains\n",
    "interim_baobab_omega_path = root_path + 'configs/baobab_configs/train_diagonal.py'\n",
    "target_ovejero_omega_path = root_path + 'configs/baobab_configs/cent_narrow_cfg_prior.py'\n",
    "target_baobab_omega_path = root_path + 'configs/baobab_configs/cent_narrow.py'\n",
    "test_dataset_path = '/Users/sebwagner/Documents/Grad_School/Research/Phil/ovejero/datasets/cent_narrow/'\n",
    "test_dataset_tf_record_path = test_dataset_path + 'tf_record_cn'\n",
    "num_samples = 1000\n",
    "n_walkers = 50\n",
    "\n",
    "# We don't need to load the models since we already have samples\n",
    "lite_class = True\n",
    "\n",
    "# The HierarchicalClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the test dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "# To initialize it we need only open up our config and pass the paths we set above.\n",
    "config_path = root_path + 'configs/nn1_hr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn1 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "\n",
    "# We can load the chains for nn1_hr on the centered narrow distribution\n",
    "save_path_samples_nn1_hr = root_path + 'hierarchical_results/cn_nn1_hr_samps/'\n",
    "hier_infer_nn1.gen_samples(num_samples,save_path_samples_nn1_hr)\n",
    "# Same goes for the samples\n",
    "n_walkers = 50\n",
    "save_path_chains_nn1_hr = root_path + 'hierarchical_results/cn_nn1_hr.h5'\n",
    "hier_infer_nn1.initialize_sampler(n_walkers,save_path_chains_nn1_hr)\n",
    "\n",
    "# Repeat this process for our remaining two distributions\n",
    "# Full Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn2_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn2 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn2_slr = root_path + 'hierarchical_results/cn_nn2_slr_samps/'\n",
    "hier_infer_nn2.gen_samples(num_samples,save_path_samples_nn2_slr)\n",
    "save_path_chains_nn2_slr = root_path + 'hierarchical_results/cn_nn2_slr.h5'\n",
    "hier_infer_nn2.initialize_sampler(n_walkers,save_path_chains_nn2_slr)\n",
    "\n",
    "# GMM Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn3_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn3 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn3_slr = root_path + 'hierarchical_results/cn_nn3_slr_samps/'\n",
    "hier_infer_nn3.gen_samples(num_samples,save_path_samples_nn3_slr)\n",
    "save_path_chains_nn3_slr = root_path + 'hierarchical_results/cn_nn3_slr.h5'\n",
    "hier_infer_nn3.initialize_sampler(n_walkers,save_path_chains_nn3_slr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly check that each of our chains are converged. These lines are commented out to avoid extending the length of notebook, but you should run them at least once to convince yourself the chains are converged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "burnin = 8000\n",
    "# hier_infer_nn1.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn1.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn2.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn2.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main parameter of interest is the power-law mass slope so we'll focus on that for the posteriors of the centered narrow distribution. First we generate the 1D plots for our three models. We do not use these in the paper so they are commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# color_map = [\"#253494\",'#1b9e77',\"#41b6c4\",\"#2c7fb8\"]\n",
    "# bnn_name = 'Diagonal BNN'\n",
    "# save_fig_path = 'figures/cn_gamma_lens_1d_diag.pdf'\n",
    "# hier_infer_nn1.plot_distributions(burnin,param_plot_names,color_map=color_map,bnn_name=bnn_name,\n",
    "#                                   plot_param='lens_mass_gamma',save_fig_path=save_fig_path)\n",
    "\n",
    "# color_map[1] = \"#d95f02\"\n",
    "# bnn_name = 'Full BNN'\n",
    "# save_fig_path = 'figures/cn_gamma_lens_1d_full.pdf'\n",
    "# hier_infer_nn2.plot_distributions(burnin,param_plot_names,color_map=color_map,bnn_name=bnn_name,\n",
    "#                                   plot_param='lens_mass_gamma',save_fig_path=save_fig_path)\n",
    "\n",
    "# color_map[1] = \"#7570b3\" \n",
    "# bnn_name = 'GMM BNN'\n",
    "# save_fig_path = 'figures/cn_gamma_lens_1d_gmm.pdf'\n",
    "# hier_infer_nn3.plot_distributions(burnin,param_plot_names,color_map=color_map,bnn_name=bnn_name,\n",
    "#                                   plot_param='lens_mass_gamma',save_fig_path=save_fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the 1d distributions, we may also be interested in comparing the posteriors for the two hyperparameters we've inferred hierarchically for gamma lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'lens_mass_gamma'\n",
    "fontsize = 13\n",
    "figure = hier_infer_nn1.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           fontsize=fontsize)\n",
    "figure = hier_infer_nn2.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           fontsize=fontsize)\n",
    "figure = hier_infer_nn3.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           fontsize=fontsize)\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'Diagonal (30%)', r'Full (0.1%)',r'GMM (0.1%)'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/cn_gamma_lens_2d_hyps.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'external_shear_gamma_ext'\n",
    "figure = hier_infer_nn1.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',fontsize=fontsize)\n",
    "figure = hier_infer_nn2.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           fontsize=fontsize)\n",
    "figure = hier_infer_nn3.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           fontsize=fontsize)\n",
    "\n",
    "# Modify the axis limits manually to get the Diagonal posterior\n",
    "figure.axes[0].set_xlim([-2.83,-2.71])\n",
    "figure.axes[2].set_xlim([-2.83,-2.71])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'Diagonal (30%)', r'Full (0.1%)',r'GMM (0.1%)'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/cn_gamma_ext_2d_hyps.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine how the hierarchical reweighting impacts the calibration of our BNN on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = np.array(['#000000','#1b9e77','#d95f02','#7570b3','#e7298a'])\n",
    "legend=[r'Perfect Calibration',r'Diagonal (30%) with Interim Prior',\n",
    "        r'Diagonal (30%) with Hierarchical Weights',r'Full (0.1%) with Interim Prior',\n",
    "        r'Full (0.1%) with Hierarchical Weights',r'GMM (0.1%) with Interim Prior',\n",
    "        r'GMM (0.1%) with Hierarchical Weights']\n",
    "ls_list =['-','--']\n",
    "n_perc_points = 30\n",
    "n_p_omega_samps = 100\n",
    "fig = hier_infer_nn1.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,1,1]],legend=None,ls_list=ls_list)\n",
    "fig = hier_infer_nn2.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,2,2]],legend=None,ls_list=ls_list,\n",
    "                                                 figure=fig)\n",
    "fig = hier_infer_nn3.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,3,3]],legend=None,ls_list=ls_list,\n",
    "                                                 figure=fig)\n",
    "\n",
    "plt.legend(legend,loc=0,framealpha=1.0,fontsize=fontsize)\n",
    "plt.title('')\n",
    "plt.xlabel('Percentage of Probability Volume',fontsize=fontsize)\n",
    "plt.ylabel('Percent of Lenses With True Value in the Volume',fontsize=fontsize)\n",
    "plt.savefig('figures/calibration_cn.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot a comparison of the input distribution to the reweighted BNN outputs. This is less informative of individual parameter performance than the plots we've made so far, but it helps form a better picture of the overall shift if the population inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of this code mirrors the code used to plot the distributions in the first panel. The only change is that now\n",
    "# we're only plotting one test distribution and we're plotting the GMM inferred distribution\n",
    "figure= None\n",
    "color_map_distributions = ['#bfbdbc','#000000','#1b9e77','#d95f02','#7570b3']\n",
    "plot_limits = None#[[0.03,0.09],[-0.3,0.3],[-0.3,0.3],[-0.15,0.15],[-0.15,0.15],[1.9,2.1],[0.95,1.05]]\n",
    "hist_limit = [60,10,10,15,15,30,40]\n",
    "# We need to smooth our test distribution because it has few samples. Note that no smoothing is applied to the \n",
    "# plotting of the inferred distribution.\n",
    "smooth = 1.2\n",
    "fontsize = 20\n",
    "hist_kwargs = {'density':True,'color':color_map_distributions[0],'lw':2}\n",
    "figure = corner.corner(train_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[0],levels=[0.68,0.95],\n",
    "                       fill_contours=True,fig=figure,hist_kwargs=hist_kwargs,smooth=smooth)\n",
    "hist_kwargs['color']=color_map_distributions[1]\n",
    "figure = corner.corner(cn_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[1],levels=[0.68,0.95],\n",
    "                       fill_contours=False,fig=figure,range=plot_limits,smooth=smooth,hist_kwargs=hist_kwargs)\n",
    "hier_infer_nn1.plot_parameter_distribtuion(burnin,lens_params,n_p_omega_samps=100,samps_per_omega=1000,figure=figure,\n",
    "                                          color=color_map_distributions[2],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names)\n",
    "hier_infer_nn2.plot_parameter_distribtuion(burnin,lens_params,n_p_omega_samps=100,samps_per_omega=1000,figure=figure,\n",
    "                                          color=color_map_distributions[3],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names)\n",
    "hier_infer_nn3.plot_parameter_distribtuion(burnin,lens_params,n_p_omega_samps=100,samps_per_omega=1000,figure=figure,\n",
    "                                          color=color_map_distributions[4],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names)\n",
    "handles = [Line2D([0], [0], color=color_map_distributions[0], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[1], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[2], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[3], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[4], lw=5)]\n",
    "figure.legend(handles,[r'Training Distribution: $p(\\xi^{\\star}|\\Omega_i)$', \n",
    "                       r'Centered Test Set: $p(\\xi^{\\star}|\\Omega_\\mathrm{true})$',\n",
    "                       r'Diagonal (30%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $',\n",
    "                       r'Full (0.1%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $',\n",
    "                       r'GMM (0.1%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $'],\n",
    "                       loc=(0.46,0.75),fontsize=fontsize)\n",
    "\n",
    "axes = np.array(figure.axes).reshape((len(lens_params),len(lens_params)))\n",
    "for i in range(len(lens_params)):\n",
    "    axes[i,i].set_ylim(0,hist_limit[i])\n",
    "    \n",
    "plt.savefig('figures/cn_distribution_comp.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance of all three model types on the shifted narrow test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal model 30% dropout chains on shifted test distribution\n",
    "# Specify the paths for our distribution configs, the test set, and the location on the saved BNN samples and chains\n",
    "interim_baobab_omega_path = root_path + 'configs/baobab_configs/train_diagonal.py'\n",
    "target_ovejero_omega_path = root_path + 'configs/baobab_configs/shifted_narrow_cfg_prior.py'\n",
    "target_baobab_omega_path = root_path + 'configs/baobab_configs/shifted_narrow.py'\n",
    "test_dataset_path = '/Users/sebwagner/Documents/Grad_School/Research/Phil/ovejero/datasets/shifted_narrow/'\n",
    "test_dataset_tf_record_path = test_dataset_path + 'tf_record_sn'\n",
    "\n",
    "# The HierarchicalClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the test dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "# To initialize it we need only open up our config and pass the paths we set above.\n",
    "config_path = root_path + 'configs/nn1_hr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn1 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn1_hr = root_path + 'hierarchical_results/sn_nn1_hr_samps/'\n",
    "hier_infer_nn1.gen_samples(num_samples,save_path_samples_nn1_hr)\n",
    "save_path_chains_nn1_hr = root_path + 'hierarchical_results/sn_nn1_hr.h5'\n",
    "hier_infer_nn1.initialize_sampler(n_walkers,save_path_chains_nn1_hr)\n",
    "\n",
    "# Repeat this process for our remaining two distributions\n",
    "# Full Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn2_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn2 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn2_slr = root_path + 'hierarchical_results/sn_nn2_slr_samps/'\n",
    "hier_infer_nn2.gen_samples(num_samples,save_path_samples_nn2_slr)\n",
    "save_path_chains_nn2_slr = root_path + 'hierarchical_results/sn_nn2_slr.h5'\n",
    "hier_infer_nn2.initialize_sampler(n_walkers,save_path_chains_nn2_slr)\n",
    "\n",
    "# GMM Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn3_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn3 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn3_slr = root_path + 'hierarchical_results/sn_nn3_slr_samps/'\n",
    "hier_infer_nn3.gen_samples(num_samples,save_path_samples_nn3_slr)\n",
    "save_path_chains_nn3_slr = root_path + 'hierarchical_results/sn_nn3_slr.h5'\n",
    "hier_infer_nn3.initialize_sampler(n_walkers,save_path_chains_nn3_slr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the chains have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "burnin = 8000\n",
    "# hier_infer_nn1.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn1.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn2.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn2.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, our main parameter of interest is the power-law mass slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# color_map = [\"#253494\",'#1b9e77',\"#41b6c4\",\"#2c7fb8\"]\n",
    "# bnn_name = 'Diagonal BNN'\n",
    "# save_fig_path = 'figures/sn_gamma_lens_1d_diag.pdf'\n",
    "# hier_infer_nn1.plot_distributions(burnin,param_plot_names,color_map=color_map,bnn_name=bnn_name,\n",
    "#                                   plot_param='lens_mass_gamma',save_fig_path=save_fig_path)\n",
    "\n",
    "# color_map[1] = \"#d95f02\"\n",
    "# bnn_name = 'Full BNN'\n",
    "# save_fig_path = 'figures/sn_gamma_lens_1d_full.pdf'\n",
    "# hier_infer_nn2.plot_distributions(burnin,param_plot_names,color_map=color_map,bnn_name=bnn_name,\n",
    "#                                   plot_param='lens_mass_gamma',save_fig_path=save_fig_path)\n",
    "\n",
    "# color_map[1] = \"#7570b3\" \n",
    "# bnn_name = 'GMM BNN'\n",
    "# save_fig_path = 'figures/sn_gamma_lens_1d_gmm.pdf'\n",
    "# hier_infer_nn3.plot_distributions(burnin,param_plot_names,color_map=color_map,bnn_name=bnn_name,\n",
    "#                                   plot_param='lens_mass_gamma',save_fig_path=save_fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the shifted distribution involves a ~3 sigma shift, our hierarchical reconstruction goes surprisingly well. It's worth looking at the same 2d contours as before to compare our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'lens_mass_gamma'\n",
    "plot_range = None#[[0.77,0.802],[0.009,0.02]]\n",
    "fontsize = 13\n",
    "figure = hier_infer_nn1.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn2.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "\n",
    "# Modify the axis limits manually to get the Diagonal posterior\n",
    "figure.axes[0].set_xlim([0.77,0.802])\n",
    "figure.axes[2].set_xlim([0.77,0.802])\n",
    "figure.axes[2].set_ylim([0.009,0.02])\n",
    "figure.axes[3].set_xlim([0.009,0.02])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'Diagonal (30%)', r'Full (0.1%)',r'GMM (0.1%)'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/sn_gamma_lens_2d_hyps.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'external_shear_gamma_ext'\n",
    "plot_range = None\n",
    "figure = hier_infer_nn1.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn2.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "\n",
    "# Modify the axis limits manually to get the Diagonal posterior\n",
    "figure.axes[0].set_xlim([-1.4,-1.29])\n",
    "figure.axes[2].set_xlim([-1.4,-1.29])\n",
    "figure.axes[2].set_ylim([0.08,0.12])\n",
    "figure.axes[3].set_xlim([0.08,0.12])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'Diagonal (30%)', r'Full (0.1%)',r'GMM (0.1%)'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "\n",
    "plt.savefig('figures/sn_gamma_ext_2d_hyps.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how this impacts our calibration as we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = np.array(['#000000','#1b9e77','#d95f02','#7570b3','#e7298a'])\n",
    "legend=[r'Perfect Calibration',r'Diagonal (30%) with Interim Prior',\n",
    "        r'Diagonal (30%) with Hierarchical Weights',r'Full (0.1%) with Interim Prior',\n",
    "        r'Full (0.1%) with Hierarchical Weights',r'GMM (0.1%) with Interim Prior',\n",
    "        r'GMM (0.1%) with Hierarchical Weights']\n",
    "ls_list =['-','--']\n",
    "n_perc_points = 30\n",
    "n_p_omega_samps = 100\n",
    "fig = hier_infer_nn1.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,1,1]],legend=None,ls_list=ls_list)\n",
    "fig = hier_infer_nn2.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,2,2]],legend=None,ls_list=ls_list,\n",
    "                                                 figure=fig)\n",
    "fig = hier_infer_nn3.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,3,3]],legend=None,ls_list=ls_list,\n",
    "                                                 figure=fig)\n",
    "\n",
    "plt.legend(legend,loc=0,framealpha=1.0,fontsize=fontsize)\n",
    "plt.title('')\n",
    "plt.xlabel('Percentage of Probability Volume',fontsize=fontsize)\n",
    "plt.ylabel('Percent of Lenses With True Value in the Volume',fontsize=fontsize)\n",
    "plt.savefig('figures/calibration_sn.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same comparison of all the distribution values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of this code mirrors the code used to plot the distributions in the first panel. The only change is that now\n",
    "# we're only plotting one test distribution and we're plotting the GMM inferred distribution\n",
    "figure= None\n",
    "color_map_distributions = ['#bfbdbc','#000000','#1b9e77','#d95f02','#7570b3']\n",
    "plot_limits = None#[[0,0.2],[-0.3,0.3],[-0.3,0.3],[-0.3,0.3],[-0.3,0.3],[1.9,2.1],[0.9,1.1]]\n",
    "hist_limit = [20,10,10,20,20,20,50]\n",
    "# We need to smooth our test distribution because it has few samples. Note that no smoothing is applied to the \n",
    "# plotting of the inferred distribution.\n",
    "smooth = 1.1\n",
    "fontsize = 20\n",
    "hist_kwargs = {'density':True,'color':color_map_distributions[0],'lw':2}\n",
    "figure = corner.corner(train_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[0],levels=[0.68,0.95],\n",
    "                       fill_contours=True,fig=figure,range=plot_limits,hist_kwargs=hist_kwargs)\n",
    "hist_kwargs['color']=color_map_distributions[1]\n",
    "figure = corner.corner(sn_samps,labels=final_params_print_names,bins=30,show_titles=False, plot_datapoints=False,\n",
    "                       label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[1],levels=[0.68,0.95],\n",
    "                       fill_contours=False,fig=figure,range=plot_limits,smooth=smooth,hist_kwargs=hist_kwargs)\n",
    "hier_infer_nn1.plot_parameter_distribtuion(burnin,lens_params,n_p_omega_samps=100,samps_per_omega=1000,figure=figure,\n",
    "                                          color=color_map_distributions[2],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names)\n",
    "hier_infer_nn2.plot_parameter_distribtuion(burnin,lens_params,n_p_omega_samps=100,samps_per_omega=1000,figure=figure,\n",
    "                                          color=color_map_distributions[3],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names)\n",
    "hier_infer_nn3.plot_parameter_distribtuion(burnin,lens_params,n_p_omega_samps=100,samps_per_omega=1000,figure=figure,\n",
    "                                          color=color_map_distributions[4],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names)\n",
    "handles = [Line2D([0], [0], color=color_map_distributions[0], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[1], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[2], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[3], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[4], lw=5)]\n",
    "figure.legend(handles,[r'Training Distribution: $p(\\xi^{\\star}|\\Omega_i)$', \n",
    "                       r'Shifted Test Set: $p(\\xi^{\\star}|\\Omega_\\mathrm{true})$',\n",
    "                       r'Diagonal (30%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $',\n",
    "                       r'Full (0.1%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $',\n",
    "                       r'GMM (0.1%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $'],\n",
    "                       loc=(0.46,0.75),fontsize=fontsize)\n",
    "\n",
    "axes = np.array(figure.axes).reshape((len(lens_params),len(lens_params)))\n",
    "for i in range(len(lens_params)):\n",
    "    axes[i,i].set_ylim(0,hist_limit[i])\n",
    "    \n",
    "plt.savefig('figures/sn_distribution_comp.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance of all three model types on the empirical test distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to load all of the bnns and their samples from the hierarchical inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal model 30% dropout chains on shifted test distribution\n",
    "# Specify the paths for our distribution configs, the test set, and the location on the saved BNN samples and chains\n",
    "interim_baobab_omega_path = root_path + 'configs/baobab_configs/train_diagonal.py'\n",
    "target_ovejero_omega_path = root_path + 'configs/baobab_configs/empirical_prior.py'\n",
    "target_baobab_omega_path = root_path + 'configs/baobab_configs/empirical.py'\n",
    "test_dataset_path = '/Users/sebwagner/Documents/Grad_School/Research/Phil/ovejero/datasets/empirical/'\n",
    "test_dataset_tf_record_path = test_dataset_path + 'tf_record_emp'\n",
    "train_to_test_param_map = dict(orig_params=['lens_mass_e1','lens_mass_e2'],transform_func=ellipticity2phi_q,\n",
    "                               new_params=['lens_mass_phi','lens_mass_q'])\n",
    "hyperparam_plot_names = [r'$\\mu_{\\log(\\gamma_\\mathrm{ext})}$',r'$\\sigma_{\\log(\\gamma_\\mathrm{ext})}$',\n",
    "                         r'$\\mu_x$',r'$\\sigma_x$',r'$\\mu_y$',r'$\\sigma_y$',\n",
    "                         r'$\\mu_{\\log(\\theta_E)}$',r'$\\mu_{\\log(q_\\mathrm{lens})}$',\n",
    "                         r'$\\mu_{\\log(\\gamma_\\mathrm{lens})}$',\n",
    "                         r'$\\Sigma_{\\log(\\theta_E),\\log(\\theta_E)}$',\n",
    "                         r'$\\Sigma_{\\log(\\theta_E),\\log(q_\\mathrm{lens})}$',\n",
    "                         r'$\\Sigma_{\\log(q_\\mathrm{lens}),\\log(q_\\mathrm{lens})}$',\n",
    "                         r'$\\Sigma_{\\log(\\theta_E),\\log(\\gamma_\\mathrm{lens})}$',\n",
    "                         r'$\\Sigma_{\\log(q_\\mathrm{lens}),\\log(\\gamma_\\mathrm{lens})}$',\n",
    "                         r'$\\Sigma_{\\log(\\gamma_\\mathrm{lens}),\\log(\\gamma_\\mathrm{lens})}$']\n",
    "lens_params_non_cov = ['external_shear_gamma_ext','lens_mass_center_x','lens_mass_center_y']\n",
    "final_params_print_names_cov = [r'$\\gamma_\\mathrm{ext}$',r'$x_\\mathrm{lens}$',r'$y_\\mathrm{lens}$',\n",
    "                                r'$\\theta_E$',r'$q_\\mathrm{lens}$',r'$\\gamma_\\mathrm{lens}$']\n",
    "# We used more walkers for the empirical distributions\n",
    "n_walkers = 200\n",
    "\n",
    "# The HierarchicalClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the test dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "# To initialize it we need only open up our config and pass the paths we set above.\n",
    "config_path = root_path + 'configs/nn1_hr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn1 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          train_to_test_param_map=train_to_test_param_map,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn1_hr = root_path + 'hierarchical_results/emp_nn1_hr_samps/'\n",
    "hier_infer_nn1.gen_samples(num_samples,save_path_samples_nn1_hr)\n",
    "save_path_chains_nn1_hr = root_path + 'hierarchical_results/emp_nn1_hr.h5'\n",
    "hier_infer_nn1.initialize_sampler(n_walkers,save_path_chains_nn1_hr)\n",
    "\n",
    "# Repeat this process for our remaining two distributions\n",
    "# Full Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn2_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn2 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          train_to_test_param_map=train_to_test_param_map,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn2_slr = root_path + 'hierarchical_results/emp_nn2_slr_samps/'\n",
    "hier_infer_nn2.gen_samples(num_samples,save_path_samples_nn2_slr)\n",
    "save_path_chains_nn2_slr = root_path + 'hierarchical_results/emp_nn2_slr.h5'\n",
    "hier_infer_nn2.initialize_sampler(n_walkers,save_path_chains_nn2_slr)\n",
    "\n",
    "# GMM Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn3_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn3 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          train_to_test_param_map=train_to_test_param_map,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn3_slr = root_path + 'hierarchical_results/emp_nn3_slr_samps/'\n",
    "hier_infer_nn3.gen_samples(num_samples,save_path_samples_nn3_slr)\n",
    "save_path_chains_nn3_slr = root_path + 'hierarchical_results/emp_nn3_slr.h5'\n",
    "hier_infer_nn3.initialize_sampler(n_walkers,save_path_chains_nn3_slr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, commented out but worth running once to make sure what you see is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "burnin = 4000\n",
    "# hier_infer_nn1.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn1.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn2.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn2.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus on the posterior plots for the covariance matrix parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'external_shear_gamma_ext'\n",
    "plot_range = None\n",
    "figures = None\n",
    "fontsize = 19\n",
    "figures = hier_infer_nn1.plot_cov_corner(burnin,hyperparam_plot_names,color='#1b9e77',figures = figures,\n",
    "                                         fontsize=fontsize)\n",
    "figures = hier_infer_nn2.plot_cov_corner(burnin,hyperparam_plot_names,color='#d95f02',figures = figures,\n",
    "                                         fontsize=fontsize)\n",
    "figures = hier_infer_nn3.plot_cov_corner(burnin,hyperparam_plot_names,color='#7570b3',figures = figures,\n",
    "                                         fontsize=fontsize)\n",
    "\n",
    "# Modify the axis limits manually to get the Diagonal posterior\n",
    "figures[0].axes[3].set_ylim([-0.51,-0.40])\n",
    "figures[0].axes[4].set_xlim([-0.51,-0.40])\n",
    "figures[0].axes[3].set_ylim([-0.51,-0.40])\n",
    "figures[0].axes[7].set_xlim([-0.51,-0.40])\n",
    "figures[1].axes[12].set_ylim([0.05,0.14])\n",
    "figures[1].axes[13].set_ylim([0.05,0.14])\n",
    "figures[1].axes[14].set_xlim([0.05,0.14])\n",
    "figures[1].axes[20].set_xlim([0.05,0.14])\n",
    "figures[1].axes[26].set_xlim([0.05,0.14])\n",
    "figures[1].axes[32].set_xlim([0.05,0.14])\n",
    "figures[1].axes[30].set_ylim([0.0025,0.005])\n",
    "figures[1].axes[31].set_ylim([0.0025,0.005])\n",
    "figures[1].axes[32].set_ylim([0.0025,0.005])\n",
    "figures[1].axes[33].set_ylim([0.0025,0.005])\n",
    "figures[1].axes[34].set_ylim([0.0025,0.005])\n",
    "figures[1].axes[35].set_xlim([0.0025,0.005])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figures[0].legend(handles,[r'Diagonal (30%)', r'Full (0.1%)',r'GMM (0.1%)'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "figures[1].legend(handles,[r'Diagonal (30%)', r'Full (0.1%)',r'GMM (0.1%)'],\n",
    "              loc=(0.575,0.75),fontsize=fontsize)\n",
    "\n",
    "\n",
    "figures[0].savefig('figures/emp_cov_mean_2d_hyps.pdf')\n",
    "figures[1].savefig('figures/emp_cov_sigma_2d_hyps.pdf')\n",
    "plt.show(figures[0])\n",
    "plt.show(figures[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the calibration of our networks. Here, we don't expect to start with as substantial of a bias as the shifted narrow distribution. We expect something closer to the center narrow distribution and that's what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = np.array(['#000000','#1b9e77','#d95f02','#7570b3','#e7298a'])\n",
    "legend=[r'Perfect Calibration',r'Diagonal (30%) with Interim Prior',\n",
    "        r'Diagonal (30%) with Hierarchical Weights',r'Full (0.1%) with Interim Prior',\n",
    "        r'Full (0.1%) with Hierarchical Weights',r'GMM (0.1%) with Interim Prior',\n",
    "        r'GMM (0.1%) with Hierarchical Weights']\n",
    "ls_list =['-','--']\n",
    "n_perc_points = 30\n",
    "n_p_omega_samps = 100\n",
    "fontsize = 13\n",
    "fig = hier_infer_nn1.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,1,1]],legend=None,ls_list=ls_list)\n",
    "fig = hier_infer_nn2.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,2,2]],legend=None,ls_list=ls_list,\n",
    "                                                 figure=fig)\n",
    "fig = hier_infer_nn3.plot_reweighted_calibration(burnin,n_perc_points,n_p_omega_samps=n_p_omega_samps,\n",
    "                                                 color_map=color_map[[0,3,3]],legend=None,ls_list=ls_list,\n",
    "                                                 figure=fig)\n",
    "\n",
    "plt.legend(legend,loc=0,framealpha=1.0,fontsize=fontsize)\n",
    "plt.title('')\n",
    "plt.xlabel('Percentage of Probability Volume',fontsize=fontsize)\n",
    "plt.ylabel('Percent of Lenses With True Value in the Volume',fontsize=fontsize)\n",
    "plt.savefig('figures/calibration_em.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the distribution here as before. The one tricky part is that we need to modify the trianing samps\n",
    "such that we are sampling q instead of e1 and e2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure= None\n",
    "color_map_distributions = ['#bfbdbc','#000000','#1b9e77','#d95f02','#7570b3']\n",
    "plot_limits = None#[[0,0.2],[-0.3,0.3],[-0.3,0.3],[-0.3,0.3],[-0.3,0.3],[1.9,2.1],[0.9,1.1]]\n",
    "hist_limit = [100,10,10,5,3,5]\n",
    "\n",
    "# Generate training and empirical samples where we have q instead of e1,e2\n",
    "train_samps_cov = np.copy(train_samps)\n",
    "te1,te2 = train_samps_cov[:,[3,4]].T\n",
    "emp_samps_cov = np.copy(emp_samps)\n",
    "ee1,ee2 = emp_samps_cov[:,[3,4]].T\n",
    "# Convert to q\n",
    "_, tq = ellipticity2phi_q(te1,te2)\n",
    "_, eq = ellipticity2phi_q(ee1,ee2)\n",
    "# Insert back into the array and do some reordering\n",
    "train_samps_cov[:,3] = tq\n",
    "train_samps_cov = np.delete(train_samps_cov,4,axis=1)\n",
    "train_samps_cov[:,[3,4,5]] = train_samps_cov[:,[5,3,4]]\n",
    "emp_samps_cov[:,3] = eq\n",
    "emp_samps_cov = np.delete(emp_samps_cov,4,axis=1)\n",
    "emp_samps_cov[:,[3,4,5]] = emp_samps_cov[:,[5,3,4]]\n",
    "\n",
    "# We need to smooth our test distribution because it has few samples. Note that no smoothing is applied to the \n",
    "# plotting of the inferred distribution.\n",
    "smooth = 1.1\n",
    "fontsize = 20\n",
    "hist_kwargs = {'density':True,'color':color_map_distributions[0],'lw':2}\n",
    "\n",
    "figure = corner.corner(train_samps_cov,labels=final_params_print_names_cov,bins=30,show_titles=False, \n",
    "                       plot_datapoints=False,label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[0],\n",
    "                       levels=[0.68,0.95],fill_contours=True,fig=figure,range=plot_limits,hist_kwargs=hist_kwargs)\n",
    "hist_kwargs['color']=color_map_distributions[1]\n",
    "figure = corner.corner(emp_samps_cov,labels=final_params_print_names_cov,bins=30,show_titles=False, \n",
    "                       plot_datapoints=False, label_kwargs=dict(fontsize=fontsize),color=color_map_distributions[1],\n",
    "                       levels=[0.68,0.95],fill_contours=False,fig=figure,range=plot_limits,smooth=smooth,\n",
    "                       hist_kwargs=hist_kwargs)\n",
    "hier_infer_nn1.plot_parameter_distribtuion(burnin,lens_params_non_cov,n_p_omega_samps=100,samps_per_omega=1000,\n",
    "                                           figure=figure,color=color_map_distributions[2],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names_cov,\n",
    "                                           cov_params=True)\n",
    "hier_infer_nn2.plot_parameter_distribtuion(burnin,lens_params_non_cov,n_p_omega_samps=100,samps_per_omega=1000,\n",
    "                                           figure=figure,color=color_map_distributions[3],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names_cov,\n",
    "                                           cov_params=True)\n",
    "hier_infer_nn3.plot_parameter_distribtuion(burnin,lens_params_non_cov,n_p_omega_samps=100,samps_per_omega=1000,\n",
    "                                           figure=figure,color=color_map_distributions[4],plot_limits=plot_limits,\n",
    "                                           fontsize=fontsize,param_print_names=final_params_print_names_cov,\n",
    "                                           cov_params=True)\n",
    "handles = [Line2D([0], [0], color=color_map_distributions[0], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[1], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[2], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[3], lw=5),\n",
    "           Line2D([0], [0], color=color_map_distributions[4], lw=5)]\n",
    "figure.legend(handles,[r'Training Distribution: $p(\\xi^{\\star}|\\Omega_i)$', \n",
    "                       r'Empirical Test Set: $p(\\xi^{\\star}|\\Omega_\\mathrm{true})$',\n",
    "                       r'Diagonal (30%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $',\n",
    "                       r'Full (0.1%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $',\n",
    "                       r'GMM (0.1%) Inferred Distribution: $p(\\xi^{\\star}|\\Omega)p(\\Omega|\\{d\\}) $'],\n",
    "                       loc=(0.40,0.75),fontsize=fontsize)\n",
    "\n",
    "axes = np.array(figure.axes).reshape((len(final_params_print_names_cov),len(final_params_print_names_cov)))\n",
    "for i in range(len(final_params_print_names_cov)):\n",
    "    axes[i,i].set_ylim(0,hist_limit[i])\n",
    "    \n",
    "plt.savefig('figures/emp_distribution_comp.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance of GMM model, varying dropout rate, on centered narrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this comparison, we are mainly only intereted in looking at the 2d posteriors on the population parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Diagonal model 30% dropout chains on shifted test distribution\n",
    "# Specify the paths for our distribution configs, the test set, and the location on the saved BNN samples and chains\n",
    "interim_baobab_omega_path = root_path + 'configs/baobab_configs/train_diagonal.py'\n",
    "target_ovejero_omega_path = root_path + 'configs/baobab_configs/cent_narrow_cfg_prior.py'\n",
    "target_baobab_omega_path = root_path + 'configs/baobab_configs/cent_narrow.py'\n",
    "test_dataset_path = '/Users/sebwagner/Documents/Grad_School/Research/Phil/ovejero/datasets/cent_narrow/'\n",
    "test_dataset_tf_record_path = test_dataset_path + 'tf_record_cn'\n",
    "n_walkers = 50\n",
    "hyperparam_plot_names = [r'$\\mu_{\\log(\\gamma_\\mathrm{ext})}$',r'$\\sigma_{\\log(\\gamma_\\mathrm{ext})}$',\n",
    "                         r'$\\mu_x$',r'$\\sigma_x$',r'$\\mu_y$',r'$\\sigma_y$',\n",
    "                         r'$\\mu_{e1}$',r'$\\sigma_{e1}$',\n",
    "                         r'$\\mu_{e2}$',r'$\\sigma_{e2}$',\n",
    "                         r'$\\mu_{\\log (\\gamma_\\mathrm{lens})}$',r'$\\sigma_{\\log (\\gamma_\\mathrm{lens})}$',\n",
    "                         r'$\\mu_{\\log (\\theta_E)}$',r'$\\sigma_{\\log (\\theta_E)}$']\n",
    "\n",
    "# The HierarchicalClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the test dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "# To initialize it we need only open up our config and pass the paths we set above.\n",
    "config_path = root_path + 'configs/nn3_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn3_slr = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                              test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                              target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                              lite_class=lite_class)\n",
    "save_path_samples_nn3_slr = root_path + 'hierarchical_results/cn_nn3_slr_samps/'\n",
    "hier_infer_nn3_slr.gen_samples(num_samples,save_path_samples_nn3_slr)\n",
    "save_path_chains_nn3_slr = root_path + 'hierarchical_results/cn_nn3_slr.h5'\n",
    "hier_infer_nn3_slr.initialize_sampler(n_walkers,save_path_chains_nn3_slr)\n",
    "\n",
    "config_path = root_path + 'configs/nn3_lr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn3_lr = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                             test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                             target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                             lite_class=lite_class)\n",
    "save_path_samples_nn3_lr = root_path + 'hierarchical_results/cn_nn3_lr_samps/'\n",
    "hier_infer_nn3_lr.gen_samples(num_samples,save_path_samples_nn3_lr)\n",
    "save_path_chains_nn3_lr = root_path + 'hierarchical_results/cn_nn3_lr.h5'\n",
    "hier_infer_nn3_lr.initialize_sampler(n_walkers,save_path_chains_nn3_lr)\n",
    "\n",
    "# GMM Model 0.1% Dropout\n",
    "config_path = root_path + 'configs/nn3.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "hier_infer_nn3 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn3 = root_path + 'hierarchical_results/cn_nn3_samps/'\n",
    "hier_infer_nn3.gen_samples(num_samples,save_path_samples_nn3)\n",
    "save_path_chains_nn3 = root_path + 'hierarchical_results/cn_nn3.h5'\n",
    "hier_infer_nn3.initialize_sampler(n_walkers,save_path_chains_nn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "burnin = 8000\n",
    "# hier_infer_nn3_slr.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3_slr.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3_lr.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3_lr.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'lens_mass_gamma'\n",
    "plot_range = None#[[0.77,0.802],[0.009,0.02]]\n",
    "fontsize=13\n",
    "figure = hier_infer_nn3.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_lr.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_slr.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "\n",
    "# Change the axis to include all the posteriors\n",
    "figure.axes[0].set_xlim([0.697,0.705])\n",
    "figure.axes[2].set_xlim([0.697,0.705])\n",
    "figure.axes[2].set_ylim([0.005,0.0135])\n",
    "figure.axes[3].set_xlim([0.005,0.0135])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'GMM (1%)', r'GMM (0.5%)',r'GMM (0.1%)'],\n",
    "              loc=(0.59,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/cn_gamma_lens_2d_hyps_r.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'external_shear_gamma_ext'\n",
    "plot_range = None#[[0.77,0.802],[0.009,0.02]]\n",
    "figure = hier_infer_nn3.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_lr.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_slr.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "\n",
    "# Change the axis to include all the posteriors\n",
    "figure.axes[2].set_ylim([0.06,0.11])\n",
    "figure.axes[3].set_xlim([0.05,0.12])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'GMM (1%)', r'GMM (0.5%)',r'GMM (0.1%)'],\n",
    "              loc=(0.59,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/cn_gamma_ext_2d_hyps_r.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance of GMM model, varying number of lenses, on centered narrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we're mainly intereted in plotting the 2d posteriors here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The HierarchicalClass will do all the heavy lifting of preparing the model from the configuration file,\n",
    "# initializing the test dataset, and providing outputs correctly marginalized over the BNN uncertainties.\n",
    "# To initialize it we need only open up our config and pass the paths we set above.\n",
    "config_path = root_path + 'configs/nn3_slr.json'\n",
    "cfg = model_trainer.load_config(config_path)\n",
    "recursive_str_checker(cfg)\n",
    "\n",
    "hier_infer_nn3 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,target_ovejero_omega_path,\n",
    "                                                          test_dataset_path,test_dataset_tf_record_path,\n",
    "                                                          target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                          lite_class=lite_class)\n",
    "save_path_samples_nn3_slr = root_path + 'hierarchical_results/cn_nn3_slr_samps/'\n",
    "hier_infer_nn3_slr.gen_samples(num_samples,save_path_samples_nn3_slr)\n",
    "save_path_chains_nn3_slr = root_path + 'hierarchical_results/cn_nn3_slr.h5'\n",
    "hier_infer_nn3_slr.initialize_sampler(n_walkers,save_path_chains_nn3_slr)\n",
    "\n",
    "# Now 256 lenses\n",
    "hier_infer_nn3_slr_256 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,\n",
    "                                                                  target_ovejero_omega_path,test_dataset_path,\n",
    "                                                                  test_dataset_tf_record_path,\n",
    "                                                                  target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                                  lite_class=lite_class)\n",
    "num_lenses = 256\n",
    "hier_infer_nn3_slr_256.gen_samples(num_samples,save_path_samples_nn3_slr,num_lenses)\n",
    "save_path_chains_nn3_slr_256 = root_path + 'hierarchical_results/cn_nn3_slr_256.h5'\n",
    "hier_infer_nn3_slr_256.initialize_sampler(n_walkers,save_path_chains_nn3_slr_256)\n",
    "\n",
    "# GMM Model 0.1% Dropout\n",
    "hier_infer_nn3_slr_64 = hierarchical_inference.HierarchicalClass(cfg,interim_baobab_omega_path,\n",
    "                                                                 target_ovejero_omega_path,test_dataset_path,\n",
    "                                                                 test_dataset_tf_record_path,\n",
    "                                                                 target_baobab_omega_path=target_baobab_omega_path,\n",
    "                                                                 lite_class=lite_class)\n",
    "num_lenses = 64\n",
    "hier_infer_nn3_slr_64.gen_samples(num_samples,save_path_samples_nn3_slr,num_lenses)\n",
    "save_path_chains_nn3_slr_64 = root_path + 'hierarchical_results/cn_nn3_slr_64.h5'\n",
    "hier_infer_nn3_slr_64.initialize_sampler(n_walkers,save_path_chains_nn3_slr_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "burnin = 8000\n",
    "# hier_infer_nn3_slr.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3_slr.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3_slr_256.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3_slr_256.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)\n",
    "\n",
    "# hier_infer_nn3_slr_64.plot_chains(burnin=burnin,hyperparam_plot_names=hyperparam_plot_names)\n",
    "# hier_infer_nn3_slr_64.plot_auto_corr(hyperparam_plot_names=hyperparam_plot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'lens_mass_gamma'\n",
    "plot_range = None#[[0.695,0.71],[0.0055,0.0175]]\n",
    "figure = hier_infer_nn3_slr_64.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_slr_256.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_slr.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "\n",
    "# Modify the axis limits manually to get all the posteriors\n",
    "figure.axes[0].set_xlim([0.695,0.71])\n",
    "figure.axes[2].set_xlim([0.695,0.71])\n",
    "figure.axes[2].set_ylim([0.0055,0.0175])\n",
    "figure.axes[3].set_xlim([0.0055,0.0175])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'GMM 64 Lenses', r'GMM 256 Lenses',r'GMM 1024 Lenses'],\n",
    "              loc=(0.55,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/cn_gamma_lens_2d_hyps_nl.pdf')\n",
    "plt.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two contours with the truth\n",
    "plot_param = 'external_shear_gamma_ext'\n",
    "plot_range = None#[[0.695,0.71],[0.0055,0.0175]]\n",
    "figure = hier_infer_nn3_slr_64.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#1b9e77',\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_slr_256.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#d95f02',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "figure = hier_infer_nn3_slr.plot_single_corner(burnin,plot_param,hyperparam_plot_names,color='#7570b3',figure=figure,\n",
    "                                           plot_range=plot_range,fontsize=fontsize)\n",
    "\n",
    "# Modify the axis limits manually to get all the posteriors\n",
    "figure.axes[0].set_xlim([-2.78,-2.7])\n",
    "figure.axes[2].set_xlim([-2.78,-2.7])\n",
    "figure.axes[2].set_ylim([0.01,0.14])\n",
    "figure.axes[3].set_xlim([0.01,0.14])\n",
    "\n",
    "# Do some nice work to make a legend\n",
    "handles = [Line2D([0], [0], color='#1b9e77', lw=5),\n",
    "           Line2D([0], [0], color='#d95f02', lw=5),\n",
    "           Line2D([0], [0], color='#7570b3', lw=5)]\n",
    "figure.legend(handles,[r'GMM 64 Lenses', r'GMM 256 Lenses',r'GMM 1024 Lenses'],\n",
    "              loc=(0.55,0.75),fontsize=fontsize)\n",
    "plt.savefig('figures/cn_gamma_ext_2d_hyps_nl.pdf')\n",
    "plt.show(figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
